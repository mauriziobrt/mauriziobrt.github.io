---
layout: post
title:  "Tutorial - Machine Learning for Artists - 2 - Deep Learning on a compositional perspective"
date:   2024-04-09 12:11:54 +0200
categories: mla
---



>  üì° [Click here to download the tutorial patch](https://drive.google.com/drive/folders/1blM0A9qKv4a4BGOEYeakunPoNdGJs2Wd?usp=drive_link) and [click here to download the presentation](https://drive.google.com/file/d/1tMuHdd01Ox9jgcvfUy_V4-hNEWGz9Wd-/view?usp=sharing)

> ‚ö†Ô∏è Max MSP is needed to follow this tutorial


---

<br>

<h1 align="right">Neural Networks </h1>

<br>

Una rete neurale √® un modello di calcolo la cui
struttura stratificata assomiglia alla struttura della rete
di neuroni nel cervello, con strati di nodi connessi.
Una rete neurale pu√≤ apprendere dai dati, quindi
pu√≤ essere addestrata a riconoscere pattern,
classificare i dati e prevedere eventi futuri.[^1]

<br>

---
<br>
<h1></h1>

<br>

Una rete neurale suddivide gli input in livelli di astrazione. Pu√≤
essere addestrata su molti esempi per riconoscere i pattern del
parlato o delle immagini, proprio come fa il cervello umano. Il
suo comportamento √® definito dal modo in cui sono collegati i
suoi singoli elementi e dalla forza, o dai pesi, di quelle
connessioni. Questi pesi vengono regolati automaticamente
durante l‚Äôaddestramento in base a una specifica regola di
addestramento finch√© la rete neurale non esegue correttamente
l'attivit√† desiderata.

---
<br>
<h1></h1>

---

Le reti neurali sono particolarmente adatte al
riconoscimento di pattern per identificare e
classificare oggetti o segnali nel parlato, nella visione
e nei sistemi di controllo. Possono anche essere
utilizzate per eseguire la previsione e la modellazione
di serie storiche.

---
<h1></h1>
---
Le reti neurali che operano su due o tre layer di
neuroni connessi sono conosciute come reti neurali
superficiali. Le reti di Deep learning possono avere
molti layer, anche centinaia. Entrambe sono
tecniche di machine learning che imparano
direttamente dai dati di input.


---


Il deep learning √® particolarmente adatto per
applicazioni di identificazione complesse come il
riconoscimento facciale, la traduzione di testi e il
riconoscimento vocale. √à anche una tecnologia
chiave utilizzata nei sistemi avanzati di assistenza
alla guida tra cui la classificazione delle corsie e il
riconoscimento della segnaletica stradale.

---

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/v5R8r-iTp6M?si=IuuZE9lbSsxBMsPb" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>

L‚Äôinteresse per le reti neurali √® presente gi√† da molto tempo.
David Tudor ne fa uso nella sua Neural Synthesis (circa
1990), creando circuiti basati sull‚Äôemulazione della struttura
neuronale. La sua sintesi √® fatta con circuitazione analogica,
utilizzando il chip Intel 80170NX Neural Processor o ETANN,
uno dei primi processori neurali commerciali. Utilizza questo
processore in una matrice di feedback, per fare emergere il
pi√π possibile le caratteristiche sonore del processo di sintesi.

<br>

Le reti neurali supervisionate sono addestrate a
produrre gli output desiderati in risposta agli input
campione, risultando particolarmente adatte per la
modellazione e il controllo di sistemi dinamici, la
classificazione di dati rumorosi e la previsione di
eventi futuri. Alcuni esempi sono le reti feedforward, a
base radiale, dinamica e learning vector quantization.

La classificazione √® un tipo di apprendimento
automatico supervisionato in cui un algoritmo
"impara" a classificare nuove osservazioni da
esempi di dati etichettati.

Utili a scopi di analisi del dato audio.

I modelli di regressione descrivono la relazione tra
una variabile di risposta (output) e una o pi√π variabili
esplicative (input).

Quando parliamo di sintesi spesso utilizziamo
modelli di regressione.

L‚Äôaddestramento delle reti neurali senza supervisione viene
eseguito lasciando che la rete neurale si adatti continuamente
ai nuovi input. Vengono utilizzate per fare deduzioni da set di
dati formati da dati di input senza risposte etichettate. √à
possibile usarle per scoprire le distribuzioni naturali, le
categorie e le relazioni di categoria all'interno dei dati.

Deep Learning Toolbox comprende due tipi di reti non
supervisionate: competitive layer e self-organizing map.

Il clustering √® un approccio di apprendimento senza
supervisione in cui le reti neurali possono essere
utilizzate per l'analisi dei dati esplorativi per trovare
pattern nascosti o raggruppamenti nei dati. Questo
processo implica il raggruppamento dei dati per
similarit√†. Tra le applicazioni dell‚Äôanalisi cluster figurano
l‚Äôanalisi della sequenza genetica, le ricerche di mercato e
il riconoscimento di oggetti.

‚Ä¢ La rete che vedremo nel tutorial si chiama multilayer
perceptron. Vengono spesso chiamate cos√¨ le reti
feedforward, reti in cui il segnale viene passato in una
sola direzione.

‚Ä¢ Nella classificazione abbiamo tanti input e pochi
output. Si tratta di un tipo di analisi del file di ingresso.

---

<br>

Tutorial - Classificazione suoni con Flucoma

<br>

---

Una rete neurale convoluzionale pu√≤ avere decine o centinaia di
layer, ciascuno dei quali apprende feature diverse di
un‚Äôimmagine. A ciascuna immagine di addestramento vengono
applicati dei filtri a diverse risoluzioni e l‚Äôoutput di ciascuna
immagine convoluta viene utilizzato come input per il layer
successivo. I filtri possono essere inizialmente feature molto
semplici, ad esempio la luminosit√† o i bordi, e diventare sempre
pi√π complessi fino a includere feature che definiscono in modo
univoco l‚Äôoggetto.

Analogamente ad altre reti neurali, una CNN √® costituita da un layer di input, un layer di output e tanti layer intermedi
nascosti.
Questi layer eseguono operazioni che alterano i dati al fine di apprendere le feature specifiche dei dati stessi. Tre dei layer
pi√π diffusi sono: la convoluzione, l‚Äôattivazione o ReLU e il pooling.
‚Ä¢ La convoluzione sottopone le immagini di input a una serie di filtri convoluzionali, ciascuno dei quali attiva
determinate feature dalle immagini.
‚Ä¢ L‚Äôunit√† lineare rettificata (ReLU) consente di eseguire un addestramento pi√π rapido ed efficace mappando i
valori negativi a zero e mantenendo quelli positivi. Questa operazione √® talvolta definita attivazione, dal momento
che solo le feature attivate vengono trasmesse al layer successivo.
‚Ä¢ Il pooling semplifica l‚Äôoutput mediante l‚Äôesecuzione di un downsampling non lineare, riducendo in tal modo il
numero di parametri che la rete deve apprendere.

Queste operazioni vengono reiterate su decine o centinaia di layer e ciascun layer impara ad identificare feature diverse.

Esempio di una rete con numerosi layer convoluzionali. A ciascuna immagine di addestramento vengono
applicati dei filtri a diverse risoluzioni e l‚Äôoutput di ciascuna immagine convoluta viene utilizzato come
input per il layer successivo.

Bias e pesi condivisi
Analogamente a una rete neurale tradizionale, una CNN possiede neuroni con pesi e bias. Il modello
apprende questi valori durante l‚Äôaddestramento e li aggiorna costantemente con ogni nuovo esempio di
addestramento. Tuttavia, nel caso delle CNN, i valori dei pesi e dei bias sono gli stessi per tutti i neuroni
nascosti in un determinato layer.
Ci√≤ significa che tutti i neuroni nascosti rilevano la stessa feature, come bordi o macchie, in diverse aree
dell‚Äôimmagine. Ci√≤ rende la rete tollerante alla traslazione di oggetti in un‚Äôimmagine. Ad esempio, una rete
addestrata a riconoscere automobili sar√† in grado di farlo indipendentemente dal tipo di automobile
presente nell‚Äôimmagine.

Dopo aver appreso le feature in numerosi layer, l‚Äôarchitettura di una CNN
passa alla classificazione.

Il penultimo layer √® un layer completamente connesso che emette un
vettore di dimensioni K dove K √® il numero di classi che la rete sar√† in
grado di prevedere. Questo vettore contiene le probabilit√† per ciascuna
classe di qualsiasi immagine classificata.

L‚Äôultimo layer dell‚Äôarchitettura CNN utilizza un layer di classificazione come
una softmax (normalizza i valori in modo che la somma di tutti i dati nella
colonna sia pari a 1) per fornire l‚Äôoutput della classificazione.

The network accepts 2 channels magnitude spectrogram as input, U-Net is constructed using 6 pairs of encoder/
decoder, final dilated convolution layer expand second last feature map into 2 channels for stereo inference.
For 4 stem track separation, we need 4 networks to achieve separation, the neural network computes probability
mask function as final output.
The encoder uses convolutional layer with stride = 2, reduce the need for max pooling, a great improvement for a
real-time system.

Batch normalization and activation is followed by the output of each convolution layer except the bottleneck of U-
Net.

The decoder uses transposed convolution with stride = 2 for upsampling, with their input concatenated with each
encoder Conv2D pair.
Worth notice, batch normalization and activation isn't the output of each encoder layers we are going to
concatenate. The decoder side concatenates just the convolution output of the layers of an encoder.


<br>

Tutorial - Spleeter Source Separation

<br>


<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/TLEwf9P_ilc?si=H7k0FiKSXIiEDLLC" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>

Hecker √® molto interessato alla relazione tra reti neurali artificiali e suono.
‚ÄúSome years ago I‚Äôve was looking a lot into particularly Iannis Xenakis and some works he did late in his life. Concerning his
use of Dynamic Stochastic Synthesis, this revealed a process that I‚Äôve been using in many many pieces since 2001
onward, and which I still use. And basically this sort of work made me to look into non-linear ways of sound synthesis. I
never could befriend myself with Cagean ideas of chance or aleatoric systems in music, so the idea of the artificial neural
network that could create some ongoing instability or change into a system was something I felt very attracted to.‚Äù
‚ÄúFor this occasion [the Systemics #2] I‚Äôve been looking into something like an actualisation or an upgrade of what David
Tudor did with his Neural Synthesis piece from 1993. That‚Äôs a very late piece for Tudor, but I‚Äôve always loved Tudor‚Äôs
electronic music pieces, and this one particularly‚Äù But with Neural Synthesis and the involvement of an artificial neural
network, the decision making processes of the virtuous artist related to compositonal structure undergoes a
great kind of complication; when then the machine is given an important role in decision making, the idea of
virtuosity is challenged. Tommi Ker√§nen created a system that accessed some of the instruments and synthesis tools i am
using most, and connects these to an artificial neural network .
The focus is to bring in non-linear functions into my works. And my primary concerns here are the complexified
timbral structures.

1935 dramatizes two trajectories of machine-listening,
analysis, and resynthesis : first is a model for audio texture
synthesis, using time-frequency scattering, developed by Vincent
Lostanlen (heard at 07:01‚Äì11:53 of the work)‚ÄúScattering.m,‚Äù
Vincent Lostanlen‚Äôs GitHub page, www.github.com/lostanlen/
scattering.m; second is an algorithm for the synthesis and
transformation of sounds from time frequency statistics,
developed by Axel R√∂bel and members of the Analysis/Synthesis
group at IRCAM, Paris (heard at 00:00‚Äì06:58 and 11:53‚Äì19:35).

The algorithmic process of the latter begins with the analysis and extraction of statistical ‚Äúdescriptors‚Äù
from a given input sound. The computer identifies structures in audio data and subsequently generates
a representative model‚Äîa statistical ‚Äúdescriptor‚Äù‚Äîof that sound. Crucially, the identification of relevant
structures is informed by psychoacoustic theories of perception; such theories are programmed into the analysis
phase of the software to ensure that descriptors are as perceptually ‚Äúrelevant‚Äù to human listening as possible. What
results is a high-dimensional representation of the original sound to subsequently drive ‚Äúrealistic‚Äù
resynthesis‚Äîor, more interestingly and more relevant to 1935, taking specific statistical descriptors
derived from the analysis of an input sound and resynthesizing the same sound with scaled weightings.
These descriptors may well correspond with relevant perceptual categories intuitive to phenomenal listening. They may
be used to resynthesize an input with fidelity and realism from the perspective of a human listener. But, behind this
capacity for representation, they often exhibit measures of abstraction and scales of resolution foreign to human
conceptual and perceptual capacities. It is precisely this capacity for abstract description that Hecker instrumentalizes
in 1935. Here, Hecker does not resynthesize input sounds to the end of ‚Äúrealistic‚Äù‚Äîthat is to say, normative‚Äî
representation per se. Instead the aim is to highlight the inner workings of machine listening‚Äîby showing that there is a
fundamental, generative difference between synthetic and human listening.

Instead, it is to investigate the abstract description derived from this process, which both
rigorously maintains certain aspects of the original and uncovers something else in the process.

1935 consists almost entirely of resynthesized sounds derived from various descriptors. Abstract
qualities originating from sounds that are no longer heard become the phantoms that inhabit
sonic matter, fusing to form states that are both potential and contingent. Each of the three
sections of 1935 facilitates this comparison through the presence of continuous sound
textures. Acting as the sonic foundation upon which various descriptors act, each texture
stands-in as a sort of cantus firmus for each section of the work. The statistical model of
the descriptor fuses with this foundation, leaving only traces of the original; crucially, through the
referencing of this underlying texture at any given moment, the listener can treat this continuous
sound field as a control through which the different statistical filters can be heard and compared.

Utilizzando una CNN a 2 dimensioni, il segnale audio √®
alterato finch√© la cross-relazione temporale tra la

mappature delle caratteristiche (features) del suo log-
spectrogram sono simili a quelle della tessitura target.

Il risultato ottenuto √® un suono sintetizzato che √® differente
dall‚Äôoriginale ma che ne mantiene le caratteristiche ed √® in
grado di riprodurre singoli eventi apparsi nel file originale.

Xtextures estrae le correlazioni delle feature da una rappresentazione
spettrale del segnale di input, applicando allo spettro del segnale
delle convoluzioni con un set di filtri randomici.

Queste correlazioni delle feature sono poi salvate e combinate
liberamente per creare un loro set applicabile poi a nuovi segnali di
input.

L‚Äôalgoritmo √® stato sviluppato per permettere la re-sintesi di tessiture
audio, ma pu√≤ essere anche applicata a suoni arbitrari.

La tecnologia utilizzata segue un principio simile a quello del trasferimento di stile applicato alle immagini (per esempio
gli effetti delle foto in stile Van Gogh). Le differenze fondamenti tra style transfer nelle immagini e textures sono 2:
Per uno style transfer efficace la posizione e inter-relazione tra gli oggetti presenti nell‚Äôimmagine non deve essere
modificata. Nella re-sintesi di textures solo le strutture locali devono essere preservate.
Per lo style transfer nelle immagini le strutture locali possono essere traslate orizzontalmente e verticalmente senza
fondamentali differenze. Nelle tessiture audio in movimento all‚Äôinterno delle dimensioni dello spazio e del tempo altera
profondamente la percezione del suono e quindi deve essere trattato differentemente.
1. For a successful style transfer for images the positions and interrelations between objects present in the images
should not be modified. For texture resynthesis only local structures should be preserved.
2. For style transfer in images local structure can be displaced horizontally and vertically without fundamental
difference. For sound textures the movements in the two dimensions of the time frequency representation have a
very different perceptual effect and need therefore to be treated differently.

Come risultato di queste differenze concettuali, la struttura della rete Xtextures √® differente rispetto a quella utilizzata per il
Neural Image Style Transfer.
Le reti utilizzate dentro Xtextures sono principalmente formate da livelli separati di CNN, ciascuno dei quali contiene un set
di filtri rettangolari inizializzati randomicamente.
A seconda della forma dei filtri nella CNN le correlazioni tra features catturano le dipendenze temporali e frequenziali.
Se scelta propriamente e applicata alle sound textures (suoni con un forte carattere randomico composto da eventi con
una correlazione limitata temporale) questi sono sufficienti a riprodurre tessiture sonore che sono percentualmente molto
simili all‚Äôoriginale senza essere la stessa.
Depending on the forms of the filters in the CNN the feature correlations capture dependencies covering different time
and frequency spans.
If properly chosen and applied to sound textures (sounds with strongly random character composed of events with time
limited correlations) these are sufficient to reproduce texture sounds that are perceptually very similar to the original without
being the same.

Se parliamo di scene sonore questo corrisponde con la percezione che uno
potrebbe avere di essere all‚Äôinterno della stessa scena sonora ma ad un tempo
differente.

Esempi di queste tessiture sono: la pioggia che cade, l‚Äôacqua che scorre, il vento
che soffia, le onde sulla spiaggia.

Per ciascuno di questi tipi di suoni scegliere un set appropriato di filtri a
convoluzione permette di riprodurre suoni che evocano la stessa scena ambientale
senza riprodurre lo stesso suono.

Quindi idea creare una serie di tessiture e poi interpolarle con DBM.

<br>
Tutorial - XTextures
<br>

Con Auditory Scene Resynthesizer Hecker porta una prospettiva sonora basata sulla
domanda, cosa sentono le macchine che gli umani non percepiscono? In questa performance
sonora esplora un grado di risoluzione percettiva non accessibile con l‚Äôascolto umano. Hecker
adotta un approccio granulare al suono, che √® trattato come una serie di elementi discontinui,
formando arcipelaghi sonori. In questo senso adotta una procedura ricorsivi di Ri-mediazione in
cui il suono continua ad essere ri-elaborato e ri-appreso dalla macchina, in un processo di
continuo rinnovamento che porta infinite versioni del suono originale. Dopo anni di lavoro in cui
l‚Äôanalisi granulare del suono e della voce √® trattata come codice e linguaggio, Hecker √®
interessato al processo inverso, la sintesi umanizzata del prodotto computazionale come base
per investigare le peculiarit√† dell‚Äôascolto individuale. Una ricostruzione che parte da una
prospettiva neurale: la codifica del suono come sensazione. Una ricerca del suono che privilegia
l‚Äôattenzione alle caratteristiche timbriche. Hecker si muove nella musica acusmatica, il suono che
crea appare come dal nulla, non avendo alcun punto di origine identificabile.

Quindi l‚Äôascoltatore √® messo nei panni della macchina che analizza il suono grano per grano e
poi cerca di realizzare un percorso tra i diversi elementi uditi.

I Dictionary-based methods (DBMs) permettono tante possibilit√† per la
trasformazione del suono, utilizzando un‚Äôanalisi simile a quanto fatto con la sintesi
granulare. Il segnale audio viene decomposto in atomi, permettendo interessanti
manipolazioni. Vengono presentati diversi approcci per la cross-synthesis e la

cross-analysis attraverso la decomposizione in atomi utilizzando dizionari di scala-
tempo-frequenza.DBM permettono una descrizione del segnale e del suo

contenuto di alto livello, permettendo un grande controllo riguardo a cosa √®
modificato e come. Attraverso questi modelli √® possibile utilizzare la
decomposizione di un segnale per influenzare quella di un altro, creando suoni
cross-sintetizzati.

[DBM link](https://composerprogrammer.com/crossanalysiscrosssynthesis.html)

<br>

Dictionary Based Methods

Tutorial DBM

‚Ä¢ https://composerprogrammer.com/crossanalysiscrosssynthesis.html
‚Ä¢ Scarica il codice C++

‚Ä¢ Apri con Xcode, installa la libreria libsndfile con brew
‚Ä¢ Se non trova il file libsndfile controlla la posizione dei file nella cartella libraries e inserisci all‚Äôinterno del
percorso richiesto i file compilati
‚Ä¢ Nel codice modifichiamo il file main, cambiamo i percorsi in modo di avere quelli del nostro dispositivo,
creiamo la cartella richiesta per i file di output
‚Ä¢ Per utilizzare le funzioni, all‚Äôinterno di main chiamiamo la funzione che vogliamo utilizzare ad esempio
InterlinkedMP();
‚Ä¢ Compiliamo e avviamo il codice, dal terminale in basso vedremo dei numeri apparire e il file verr√† generato.

<br>


<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/e3VJ-I5Wxl4?si=vq4TowUmOU9XQJWI" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>
Wavelet Scattering

La Joint time-frequency scattering (JTFS) √® un operatore convoluzionale nel dominio
temporale-frequenziale che estrae le modulazione spettrotemporali a diverse scale e
frequenze. Offre un modello idealizzato dei spectrotemporal receptive fields (STRF)
nell corteccia uditiva primaria, quindi potrebbe servire ad un plausibile surrogato per
il giudizio sulla percezione umana su scale di eventi sonori isolati. Finora per√≤ JTFS e
STRF sono rimasti all‚Äôesterno dei tools standard per le misure della similarit√†
percettiva e metodi di valutazione per la generazione di audio. Questo √® dovuto a tre
limitazioni: differenziabilit√† (cio√® si pu√≤ ottenere la derivata), velocit√† e flessibilit√†.

Le JTFS sono utili in 3 applicazioni: apprendimento manifold non supervisionato di
modulazioni spettrotemporali, classificazione supervisionata di strumenti musicali,
risintesi di textures da suoni acustici.

La time-frequency scattering √® una trasformazione matematica delle onde sonore. Il principio fondante √®
quello di emulare il modo il cui l‚Äôapparato uditivo umano estrae informazioni dal paesaggio circostante. Il
contesto √® quello di migliorare l‚Äôintelligenza artificiale per quanto riguarda l‚Äôapprendimento di suoni, √® stata
utilizzata con successo in applicazioni di trascrizione di testo dal parlato e nel riconoscimento di suoni
urbani e musicali.

Il processo della time-frequency scattering consiste in una catena di due operazioni separate, chiamati
layers. Da una parte un layer emula il funzionamento della coclea, decomponendo le frequenze
acustiche dal basso verso l‚Äôalto. Dall‚Äôaltra il secondo livello emula le funzione dell‚Äôapparato uditivo
primario, analizzando le variazioni del contenuto acustico attraverso il tempo tra le vicine frequenze
acustiche. Queste variazioni acustiche sono definite come modulazioni spettrotemporali.
Ci√≤ che distingue la time-frequency scattering dalle generazioni precedenti di modelli computazionali di
ascolto √® che non restringe la scale delle modulazioni spettrotemporali ad un singolo valore predefinito;
ma invece, lo misura a differenti scale temporali, che variano da 1 millisecondo a diversi secondi.

L‚Äôidea originale del compositore Florian Hecker era di trasferire l‚Äôapplicazione del dominio del
time-frequency scattering dall‚Äôanalisi del suono alla sintesi, mantenendo allo stesso tempo
un‚Äôarchitettura a 2 livelli. Una delle sfide principali √® stata quella di sviluppare un algoritmo
per invertire la procedura di analisi. Nel 2015, Vincent Lostanlen risolse il problema
dell‚Äôinversione dello scattering tempo-frequenza collegandolo con la teoria delle reti neurali.
Egli ha utilizzato il metodo per l‚Äôapprendimento delle reti neurali, chiamato gradient
backpropagation, applicandolo alla catena di layer dello scattering tempo-frequenza.

Una volta che la gradient backpropagation √® diventata parte della sua libreria software (ora

kymat.io), √® diventato possibile per tutti non solo calcolare i coefficienti di scattering tempo-
frequenza associati ad un suono, ma anche reciprocamente, ottenere un segnale inverso

generato da questa analisi, il quale segnale contiene l‚Äôessenza del suono analizzato.

Il modus operandi di Florian Hecker nel suo remix consiste di due step. In primis, isola il
loop melodico di XAllegroX di Lorenzo Senni e calcola i coefficienti di scattering. Poi, utilizza
l‚Äôalgoritmo di gradient backpropagation per riottenere il loop melodico originale. Dato che
l‚Äôalgoritmo gradient backpropagation √® iterativo, non √® in grado di convertire lo scattering

tempo-frequenza in un solo step; invece, l‚Äôalgoritmo inizia da un segnale randomico e auto-
regolandosi ad ogni iterazione lentamente si avvicina al suono originale.

Come conseguenza di questo processo iterativo, appare che il gradient descent (training
del modello) costituisce un processo interessante per modellare tessiture sonore attraverso
il tempo. Infatti, dal rumore randomico iniziale, iniziano ad emergere ad ogni iterazione le
modulazioni spettro-temporali. Durante le prime iterazioni il segnale generato sar√† simile ad
un drone, ma con il procedere delle iterazioni inizia a rielaborare il contenuto ritmico iniziale,
producendo un senso di ascesa sonora, tipico delle strutture della musica dance.

La peculiarit√† dello scattering.m remix √® che l‚Äôascesa musicale prodotta non avviene
attraverso gli attributi sonori classici di frequenza e ampiezza (come nell‚ÄôEDM), ma
sotto forma di texture. Quindi lo sviluppo della gradient backpropagation per lo
scattering tempo-frequenza apre nuove strade per gli effetti audio digitali: oltre a
remixare l‚Äôampiezza (con l‚Äôeq) e la frequenza (phase-vocoder), √® possibile ora anche
remixare la texture stessa, indipendentemente dall‚Äôampiezza e dalla frequenza.

Sulle stesse linee della modulazione di ampiezza e frequenza (AM e FM), Lostanlen
propone una nuova trasformazione musicale definita meta-modulazione (MM),
perch√© opera sulla modulazione spettro-temporale invece che direttamente sul
contenuto acustico. Lo studio della meta-modulazione, sia dal punto di vista
compositivo che matematico, √® centrale nelle collaborazioni tra Lostanlen e Hecker.

<br>

Tutorial kymat.io Time Frequency Scattering nello stile di Hecker

<br>

<style>
.video-holder {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;
  overflow: hidden;
}
.video-holder iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}
</style>
<div class="video-holder">
  <iframe width="560"
          height="315" 
          src="https://www.youtube.com/embed/CZ7R3M64FiI?si=kg_8MPUoVw7vxj4k" 
          frameborder="0" 
          allowfullscreen></iframe>
</div>

‚ñ† Seriation Input

‚Ä¢ Formulation (2015)

‚Ä¢ Formulation DBM Self (2015‚Äì2017)

‚Ä¢ Formulation As Texture [hcross] (2017)

‚Ä¢ Formulation Chim 111 [hcross] (2017)

Scattergrams di seriation input, seriated output e process diagrams: Vincent Lostanlen
Matrice di correlazione del seriation input: Axel R√∂bel

Synopsis Seriation, release di Hecker √® basata sulle ricerche nel machine listening e music information
retrieval, dove il ghost in the machine, un operatore non supervisionato estrae delle feature uditive dal
segnale.
In A Script for Machine Synthesis (EMEGO 226, 2017), il terzo capitolo della trilogia di brani audio-testuali
creati in collaborazione con Reza Negarastani, allo stesso tempo un modello vocale risintetizzato e uno
generato dal computer sono modellati sulla voce del narratore, riflettendo un sistema di linguaggio,
automata e sintesi chimerizzata.

Articula√ß√£o Sintetico (EMEGO 180C, 2017) ‚Äî una risintesi completa di Articula√ß√£o (EMEGO 180, 2014)
‚Äî contiene modelli vocali sintetici di Joan La Barbara, Sugata Bose e Anna Kohler.
Central to Inspection II (EMEGO 268 / UF047, 2019) utilizza una voce generata al computer che recita
un libretto di Robin Mackay‚Äî attraverso una rete neurale e computazione di machine listening, in modo
percettivo unendo le anticipazioni formali dell‚Äôanalisi audio con gli artefatti inaspettati della sintesi.

Synopsis Seriation dramatize il suono sintetico in tutte le sue intensit√† e dettagli
trasformandoli in brani multi-canale composti da Hecker a partire dal 2015. I brani analizzati
vengo decomposti e ricostruiti utilizzando l‚Äôinformation geometry, una branca della
matematica sull‚Äôinterazione tra statistica e geometria differenziale, sviluppata da Vincent
Lostanlen.

Le similarit√† e segmentazioni logiche, parzialmente accessibili all‚Äôascoltatore umano, e
parzialmente esclusive degli agenti virtuali di ascolto, apre un dialogo tra questi operatori
spettrali.

Muovendosi tra analisi e sintesi, rendono udibile il segno dell‚ÄôIA, lasciando tracce
dell‚Äôascolto non umano, tra modelli discriminativi e generativi. Una sinossi delle architetture
analitiche e sensazioni risintetizzate.

Questo arrangiamento dei brani svolto in Synopsis Seriation, astrae
ulteriormente e distoglie l‚Äôapparizione di specifici motivi, sequenze e
caratteri, in uno sguardo allucinato. Il ricordo di quanto appena sentito,
nelle cui formulazioni e modelli di sintesi continua a navigare tra un
registro sensibile e altamente formulato.

Synopsis Seriation abbraccia la successione temporale, del suono e
dell‚Äôimmateriale, attraverso la moltitudine di prospettive uditive e
logiche codificate che mettono a dura prova la tradizionale percezione
sinottica della struttura analitica e della sensazione risintetizzata.




[^1]: This is the footnote. 